import json
import os

import requests  # Still imported but not directly used for local SD
import torch
# Import Stable Diffusion Image-to-Image pipeline
from diffusers import (  # <--- ADD StableDiffusionImg2ImgPipeline
    StableDiffusionImg2ImgPipeline, StableDiffusionPipeline)
from PIL import Image

# --- Configuration ---
# Path to the metadata generated by your GAN generation script (e.g., generate_img.py)
metadata_filepath = './generated_images/generated_images_metadata.json'

refined_output_dir = './refined_images'
os.makedirs(refined_output_dir, exist_ok=True) # Ensure output dir exists early

# Stable Diffusion specific settings
sd_model_id = "runwayml/stable-diffusion-v1-5" # Or "stabilityai/stable-diffusion-xl-base-1.0"

# NEW/ADJUSTED: Refinement Parameters for "clear but not too much clear" and "near about input"
IMG2IMG_STRENGTH = 0.55       # <--- ADJUST THIS for "near about input" and "not too much clear"
                              # Lower (e.g., 0.4-0.6) keeps closer to original. Higher (0.7-0.9) transforms more.
NUM_INFERENCE_STEPS = 40      # Moderate steps for good detail without excessive processing (e.g., 30-60)
GUIDANCE_SCALE = 8.0          # Moderate CFG for prompt adherence without being too rigid (e.g., 7.0-10.0)
NEGATIVE_PROMPT = "blurry, grainy, low quality, deformed, ugly, distorted, noise, jpeg artifacts, text, signature, watermark"
OUTPUT_HEIGHT = 512           # Desired output height
OUTPUT_WIDTH = 512            # Desired output width


device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device for Stable Diffusion: {device}")


# --- Initialize Stable Diffusion Image-to-Image Pipeline ---
print(f"Loading Stable Diffusion Image-to-Image model: {sd_model_id}...")
try:
    if "xl" in sd_model_id:
        # For SDXL, use float16 if you have less VRAM
        pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(sd_model_id, torch_dtype=torch.float16)
    else:
        # For v1.5, use the standard pipeline
        pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(sd_model_id) # <--- CHANGED TO IMG2IMG PIPELINE

    pipeline.to(device) # Move model to GPU if available
    print("Stable Diffusion model loaded successfully.")
except Exception as e:
    print(f"Error loading Stable Diffusion model: {e}")
    print("Please ensure you have enough VRAM (e.g., 8GB+ for v1.5, 12GB+ for SDXL).")
    print("If running on CPU, it will be very slow.")
    exit() # Exit if model can't be loaded


# --- Enhanced Prompt Function (tuned for clarity but not 'overly' clear) ---
def enhance_prompt(original_caption):
    # Re-introducing some balanced quality tags. Avoid "8K", "masterpiece" if you want
    # to avoid overly sharp/artificial looks, but keep "detailed" for clarity.
    enhanced_prompt = (
        f"{original_caption.strip()}, "
        "detailed, high quality, sharp focus, vibrant colors, clear, "
        "natural lighting, good composition." # Added more subtle quality tags
    )
    return enhanced_prompt

def refine_images():
    print("starting image refinement process...")

    try:
        print(f"Saving refined images to: {refined_output_dir}")

        # --- LOAD DATA FROM METADATA FILE ---
        if not os.path.exists(metadata_filepath):
            print(f"Error: Metadata file not found at {metadata_filepath}.")
            print("Please ensure your GAN generation script (e.g., generate_img.py) creates this file.")
            # IMPORTANT: For img2img, these image_path MUST point to actual, existing images!
            # These are fallback examples; ensure they exist if metadata is missing.
            example_data_from_first_stage = [
                 {"image_path": "./generated_images/generated_image_1.png", "original_caption": "A dog playing in the grass."},
                 {"image_path": "./generated_images/generated_image_2.png", "original_caption": "A bird sitting on a branch."},
                 {"image_path": "./generated_images/test.png", "original_caption": "A red ball."},
            ]
            # If you want to strictly require the metadata file, uncomment the next line:
            # return
        else:
            with open(metadata_filepath, 'r') as f:
                example_data_from_first_stage = json.load(f)
            print(f"Successfully loaded {len(example_data_from_first_stage)} entries from {metadata_filepath}.")
        # --- END LOAD DATA ---

        if not example_data_from_first_stage:
            print("No images to refine based on metadata. Exiting.")
            return

        for i, item in enumerate(example_data_from_first_stage):
            original_caption = item['original_caption']
            input_image_path = item['image_path'] # <--- GET ORIGINAL IMAGE PATH

            if not os.path.exists(input_image_path):
                print(f"Warning: Original image not found at {input_image_path}. Skipping this entry.")
                continue

            try:
                # Load the input image
                init_image = Image.open(input_image_path).convert("RGB") # Ensure RGB format
                # Optional: Resize the input image to 512x512 if it's not already
                # This ensures consistency for img2img, especially if GAN output was 64x64
                if init_image.size != (OUTPUT_WIDTH, OUTPUT_HEIGHT):
                    init_image = init_image.resize((OUTPUT_WIDTH, OUTPUT_HEIGHT), Image.LANCZOS) # Use LANCZOS for high quality down/upsampling

                print(f"Loaded input image from: {input_image_path}")

                enhanced_caption = enhance_prompt(original_caption)
                print(f"\nRefining image for original caption: '{original_caption}'")
                print(f"Using enhanced prompt: '{enhanced_caption}'")
                print(f"Using input image: {input_image_path}")
                print(f"Parameters: Strength={IMG2IMG_STRENGTH}, Steps={NUM_INFERENCE_STEPS}, Guidance={GUIDANCE_SCALE}")


                # Generate image using Stable Diffusion Image-to-Image
                generated_image = pipeline(
                    prompt=enhanced_caption,
                    negative_prompt=NEGATIVE_PROMPT, # <--- Added negative prompt
                    image=init_image,                # <--- Input image for img2img
                    strength=IMG2IMG_STRENGTH,       # <--- Strength parameter
                    num_inference_steps=NUM_INFERENCE_STEPS, # <--- Number of steps
                    guidance_scale=GUIDANCE_SCALE,   # <--- Guidance scale
                    height=OUTPUT_HEIGHT,            # <--- Desired output size
                    width=OUTPUT_WIDTH
                ).images[0] # Get the first image from the batch

                # Save the image
                refined_img_filename = f'refined_image_{i+1}_from_{os.path.basename(input_image_path)}'
                refined_img_filepath = os.path.join(refined_output_dir, refined_img_filename)
                generated_image.save(refined_img_filepath)
                print(f"Refined image saved as {refined_img_filename}")

            except Exception as e:
                print(f"Error refining image for '{original_caption}' (from {input_image_path}): {e}")
                print("Skipping this image and continuing...")

        print("\nRefinement process complete!")

    except Exception as e: # This is the outer try-except block
        print(f"An unexpected error occurred in refine_images: {e}")

if __name__ == '__main__':
    refine_images()