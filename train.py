import os

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from utils.data_loader import DataLoader

from models.discriminator import Discriminator
from models.generator import Generator

# --- Hyperparameters ---
latent_dim = 100
embedding_dim = 256        # Dimension for word embeddings
caption_encoder_hidden_size = 256 # Output dimension of your text encoder in Generator/Discriminator
image_shape = (3, 64, 64)  # (channels, height, width)
batch_size = 32
epochs = 700
max_caption_length = 50    # Must match the one in DataLoader

# --- Device Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- DataLoader Initialization ---
print("Attempting to initialize DataLoader...")
data_loader = DataLoader(
    'data/images',
    'data/captions/captions.txt',
    image_size=(64, 64),
    min_word_freq=5
)
print("DataLoader initialized successfully!")

# Debug prints (from previous interactions)
print(torch.Size([1, 1]))
print(torch.Size([2, 3, 64, 64]))

# --- Model Initialization ---
# Ensure your Generator and Discriminator models are updated to accept
# vocab_size, embedding_dim, and caption_encoder_hidden_size for text processing.
# They should handle the embedding and text encoding internally.
generator = Generator(
    noise_dim=latent_dim,
    vocab_size=data_loader.vocab_size,
    embedding_dim=embedding_dim,
    caption_encoder_hidden_size=caption_encoder_hidden_size, # This is the output dim of text encoder
    output_channels=image_shape[0] # Usually 3 for RGB
).to(device) # Move generator to device

discriminator = Discriminator(
    image_shape=image_shape,
    vocab_size=data_loader.vocab_size, # Discriminator also needs vocab to embed captions
    embedding_dim=embedding_dim,
    caption_encoder_hidden_size=caption_encoder_hidden_size # This is the output dim of text encoder
).to(device) # Move discriminator to device

# --- Loss Function ---
adversarial_loss = nn.BCELoss().to(device) # Move loss to device

# --- Optimizers ---
optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

# --- Training Loop ---
num_batches = len(data_loader.captions) // batch_size
if num_batches == 0:
    print("Warning: Not enough data to form a single batch. Check your dataset size and batch_size.")
    exit() # Exit if no batches can be formed

print(f"Total batches per epoch: {num_batches}")

for epoch in range(epochs):
    for i in range(num_batches): # Use 'i' for batch index
        # --- Get a batch of images and encoded captions ---
        batch_images_np, batch_encoded_captions_np = data_loader.get_batch(batch_size, max_caption_length)

        # Skip if get_batch returned empty data (e.g., due to missing images)
        if batch_images_np.size == 0 or batch_encoded_captions_np.size == 0:
            print(f"Skipping batch {i} in Epoch {epoch}: No data returned from data_loader.get_batch.")
            continue

        # --- Convert to PyTorch tensors and move to device ---
        # Images: (batch_size, H, W, C) -> (batch_size, C, H, W) and float
        real_imgs = torch.FloatTensor(batch_images_np).permute(0, 3, 1, 2).to(device)
        # Captions: (batch_size, max_len) and long integers for embeddings
        batch_captions_tensor = torch.LongTensor(batch_encoded_captions_np).to(device)

        # Adversarial ground truths
        valid = torch.ones(batch_size, 1).to(device)
        fake = torch.zeros(batch_size, 1).to(device)

        # -----------------
        #  Train Generator
        # -----------------
        optimizer_G.zero_grad()

        # Sample noise vector 'z'
        z = torch.randn(batch_size, latent_dim).to(device)

        # Generate a batch of new images using noise AND encoded captions
        # The generator's forward method should now accept both 'z' and 'batch_captions_tensor'
        gen_imgs = generator(z, batch_captions_tensor)

        # Loss measures generator's ability to fool the discriminator
        # Discriminator should also take generated image AND caption features
        validity = discriminator(gen_imgs, batch_captions_tensor)
        g_loss = adversarial_loss(validity, valid)

        g_loss.backward()
        optimizer_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------
        optimizer_D.zero_grad()

        # Measure discriminator's ability to classify real from generated samples
        # Real images and captions
        real_loss = adversarial_loss(discriminator(real_imgs, batch_captions_tensor), valid)
        
        # Fake images (generated by G.detach() to prevent G from learning from D's loss)
        # and their corresponding captions
        fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), batch_captions_tensor), fake)
        
        d_loss = (real_loss + fake_loss) / 2

        d_loss.backward()
        optimizer_D.step()

        # Print progress per batch
        if i % 100 == 0: # Print every 100 batches to avoid excessive output
            print(f"Epoch [{epoch}/{epochs}], Batch [{i}/{num_batches}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}")
        
        if (epoch + 1) % 10 == 0: # Save every 10 epochs
            os.makedirs('saved_models', exist_ok=True)
            torch.save(generator.state_dict(), f'saved_models/generator_epoch_{epoch+1}.pth')
            print(f"Saved generator model checkpoint at epoch {epoch+1}")

        
        
os.makedirs('saved_models', exist_ok=True) # Ensure directory exists
torch.save(generator.state_dict(), 'saved_models/generator_final.pth')
print("Saved final generator model to saved_models/generator_final.pth")
print("Training loop completed.")